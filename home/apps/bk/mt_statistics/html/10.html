<!doctype html>
<html>
<head>
  <meta http-equiv='Content-Type' content='text/html; charset=UTF-8'>
  <meta name='viewport' content='width=device-width, initial-scale=1.0, user-scalable=1.0'>
  <title>Statistics</title>
  <link rel='stylesheet' type='text/css' media='screen' href='../../../../styles/global.css'>
  <script type='text/javascript' src='../../../../scripts/global.js'></script>
  <script>MathJax={tex: {inlineMath: [['$', '$'], ['\\(', '\\)']], processEscapes: true}};</script>
  <script id='MathJax-script' async src='../../../../scripts/MathJax/tex-chtml.js'></script>
</head>
<body>
<div id='idPanel'>
<div id='idTopbar'>
  <div id='idTopbarNavigation'>
    <a href='../../../../index.html'><span class='clNavHome'><span></a>
    <a href='../../index.html'><span class='clNavIndex'><span></a>
    <a href='../index.html'><span class='clNavUp'><span></a>
    <a href='#top'><span class='clNavContent'><span></a>
  </div>
</div>
<div id='idContent'>

<a name='top'></a><br>

<a href='javascript:fncShowHide("idDiv00")'>Table of Contents</a>
<div id='idDiv00'>
<br>
<a href='#chapter_10'>       10. Continuous Random Variables</a><br>
<br>
<a href='#section_39'>       10.1. The Normal Curve</a><br>
<a href='#concept_187'>      10.1.1. Continuous Probability Distributions</a><br>
<a href='#concept_188'>      10.1.2. The Uniform Distribution</a><br>
<a href='#concept_189'>      10.1.3. The Exponential Distribution</a><br>
<a href='#concept_190'>      10.1.4. The Normal Distribution</a><br>
<a href='#concept_191'>      10.1.5. Graphing the Normal Distribution</a><br>
<a href='#concept_192'>      10.1.6. The Standard Normal Curve</a><br>
<a href='#concept_193'>      10.1.7. Finding the Area Under the Normal Curve</a><br>
<br>
<a href='#section_40'>       10.2. Normal Approximation</a><br>
<a href='#concept_194'>      10.2.1. The Normal Approximation to the Binomial Distribution</a><br>
<a href='#concept_195'>      10.2.2. The Scope of the Normal Approximation</a><br>
<a href='#concept_196'>      10.2.3. Calculating a Normal Approximation</a><br>
<a href='#concept_197'>      10.2.4. Change of Scale</a><br>
<br>
<a href='#section_41'>       10.3. Measurement Error</a><br>
<a href='#concept_198'>      10.3.1. Bias</a><br>
<a href='#concept_199'>      10.3.2. Chance Error</a><br>
<a href='#concept_200'>      10.3.3. Outliers</a><br>
<br>
<a href='#section_42'>       10.4. Expected Value and Standard Error</a><br>
<a href='#concept_201'>      10.4.1. Expected Value</a><br>
<a href='#concept_202'>      10.4.2. Standard Error</a><br>
<br>
<a href='#section_43'>       10.5. Normal Approximation for Probability Histograms</a><br>
<a href='#concept_203'>      10.5.1. Probability Histograms</a><br>
<a href='#concept_204'>      10.5.2. Probability Histograms and the Normal Curve</a><br>
<a href='#concept_205'>      10.5.3. Conclusion</a><br>
</div>

<h1 id='chapter_10'>10. Continuous Random Variables</h1>

<h2 id='section_39'>10.1. The Normal Curve</h2>

<h3 id='concept_187'>10.1.1. Continuous Probability Distributions</h3>

<blockquote>A continuous probability distribution is a representation of a variable that can take a continuous range of values.</blockquote>

<h4>Learning Objective</h4>

<p>Explain probability density function in continuous probability distribution</p>

<h4>Key Points</h4>

<ul>
<li>A probability density function is a function that describes the relative likelihood for a random variable to take on a given value.</li>
<li>Intuitively, a continuous random variable is the one which can take a continuous range of values — as opposed to a discrete distribution, where the set of possible values for the random variable is at most countable.</li>
<li>While for a discrete distribution an event with probability zero is impossible (e.g. rolling 3 and a half on a standard die is impossible, and has probability zero), this is not so in the case of a continuous random variable.</li>
</ul>

<h4>Key Term</h4>

<dl>
<dt>Lebesgue measure</dt>
<dd>The unique complete translation-invariant measure for the <equation>$\sigma$</equation>-algebra which contains all <equation>$k$</equation>-cells—in and which assigns a measure to each <equation>$k$</equation>-cell equal to that <equation>$k$</equation>-cell's volume (as defined in Euclidean geometry: i.e., the volume of the <equation>$k$</equation>-cell equals the product of the lengths of its sides).</dd>
</dl>

<p>A continuous probability distribution is a probability distribution that has a probability density function. Mathematicians also call such a distribution "absolutely continuous," since its cumulative distribution function is absolutely continuous with respect to the Lebesgue measure <span class='clFormula'>$\lambda$</span>
. If the distribution of <span class='clFormula'>$X$</span>
is continuous, then <span class='clFormula'>$X$</span>
is called a continuous random variable. There are many examples of continuous probability distributions: normal, uniform, chi-squared, and others.</p>
<p>Intuitively, a continuous random variable is the one which can take a continuous range of values—as opposed to a discrete distribution, in which the set of possible values for the random variable is at most countable. While for a discrete distribution an event with probability zero is impossible (e.g. rolling 3 and a half on a standard die is impossible, and has probability zero), this is not so in the case of a continuous random variable.</p>
<p>For example, if one measures the width of an oak leaf, the result of 3.5 cm is possible; however, it has probability zero because there are uncountably many other potential values even between 3 cm and 4 cm. Each of these individual outcomes has probability zero, yet the probability that the outcome will fall into the interval (3 cm, 4 cm) is nonzero. This apparent paradox is resolved given that the probability that <span class='clFormula'>$X$</span>
attains some value within an infinite set, such as an interval, cannot be found by naively adding the probabilities for individual values. Formally, each value has an infinitesimally small probability, which statistically is equivalent to zero.</p>
<p>The definition states that a continuous probability distribution must possess a density; or equivalently, its cumulative distribution function be absolutely continuous. This requirement is stronger than simple continuity of the cumulative distribution function, and there is a special class of distributions—singular distributions, which are neither continuous nor discrete nor a mixture of those. An example is given by the <em>Cantor distribution</em>. Such singular distributions, however, are never encountered in practice.</p>

<h3>Probability Density Functions</h3>

<p>In theory, a probability density function is a function that describes the relative likelihood for a random variable to take on a given value. The probability for the random variable to fall within a particular region is given by the integral of this variable's density over the region. The probability density function is nonnegative everywhere, and its integral over the entire space is equal to one.</p>
<p>Unlike a probability, a probability density function can take on values greater than one. For example, the uniform distribution on the interval <span class='clFormula'>$\left[0, \frac{1}{2}\right]$</span>
has probability density <span class='clFormula'>$f(x) = 2$</span>
for <span class='clFormula'>$0 \leq x \leq \frac{1}{2}$</span>
and <span class='clFormula'>$f(x) = 0$</span>
elsewhere. The standard normal distribution has probability density function:</p>
<p> <span class='clFormula'>$\displaystyle f(x) = \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}x^2}$</span>
.</p>



<a href='../images/18093.svg'><img class='clImageThumb' src='../images/18093.svg' alt='Boxplot Versus Probability Density Function'></a>

<div><b><i>Boxplot Versus Probability Density Function</i></b></div>
<p><i>Boxplot and probability density function of a normal distribution <span class='clFormula'>$$</span><span class='clFormula'>$N(0, 2)$</span>
.</i></p>

<h3 id='concept_188'>10.1.2. The Uniform Distribution</h3>

<blockquote>The continuous uniform distribution is a family of symmetric probability distributions in which all intervals of the same length are equally probable.</blockquote>

<h4>Learning Objective</h4>

<p>Contrast sampling from a uniform distribution and from an arbitrary distribution</p>

<h4>Key Points</h4>

<ul>
<li>The distribution is often abbreviated <span class='clFormula'>$U(a, b)$</span>
, with <span class='clFormula'>$a$</span>
and <span class='clFormula'>$b$</span>
being the maximum and minimum values.</li>
<li>The notation for the uniform distribution is: <span class='clFormula'>$X \sim U(a, b)$</span>
where <span class='clFormula'>$a$</span>
 is the lowest value of <span class='clFormula'>$x$</span>
and <span class='clFormula'>$b$</span>
 is the highest value of <span class='clFormula'>$x$</span>
.</li>
<li>If <span class='clFormula'>$u$</span>
is a value sampled from the standard uniform distribution, then the value <span class='clFormula'>$a + (b-a)u$</span>
follows the uniform distribution parametrized by <span class='clFormula'>$a$</span>
and <span class='clFormula'>$b$</span>
.</li>
<li>The uniform distribution is useful for sampling from arbitrary distributions.</li>
</ul>

<h4>Key Terms</h4>

<dl>
<dt>cumulative distribution function</dt>
<dd>The probability that a real-valued random variable <equation>$X$</equation> with a given probability distribution will be found at a value less than or equal to <equation>$x$</equation>.</dd>
<dt>Box–Muller transformation</dt>
<dd>A pseudo-random number sampling method for generating pairs of independent, standard, normally distributed (zero expectation, unit variance) random numbers, given a source of uniformly distributed random numbers.</dd>
<dt>p-value</dt>
<dd>The probability of obtaining a test statistic at least as extreme as the one that was actually observed, assuming that the null hypothesis is true.</dd>
</dl>

<p>The continuous uniform distribution, or rectangular distribution, is a family of symmetric probability distributions such that for each member of the family all intervals of the same length on the distribution's support are equally probable. The support is defined by the two parameters, <span class='clFormula'>$a$</span>
and <span class='clFormula'>$b$</span>
, which are its minimum and maximum values. The distribution is often abbreviated <span class='clFormula'>$U(a, b)$</span>
. It is the maximum entropy probability distribution for a random variate <span class='clFormula'>$X$</span>
under no constraint other than that it is contained in the distribution's support.</p>
<p>The probability that a uniformly distributed random variable falls within any interval of fixed length is independent of the location of the interval itself (but it is dependent on the interval size), so long as the interval is contained in the distribution's support.</p>
<p>To see this, if <span class='clFormula'>$X \sim U(a, b)$</span>
and <span class='clFormula'>$[x, x+d]$</span>
is a subinterval of <span class='clFormula'>$[a, b]$</span>
with fixed <span class='clFormula'>$d&gt;0$</span>
, then, the formula shown:</p>

<div class='clFormula'>$\displaystyle {f(x) = \begin{cases} \frac { 1 }{ b-a } &amp;\text{for } a\le x\le b \\ 0 &amp; \text{if } x \; \text{&lt;} \; a \; \text{or} \; x \; \text{&gt;} \; b \end{cases}}$</div>

<p>Is independent of <span class='clFormula'>$x$</span>
. This fact motivates the distribution's name.</p>

<h3>Applications of the Uniform Distribution</h3>

<p>When a <span class='clFormula'>$p$</span>
-value is used as a test statistic for a simple null hypothesis, and the distribution of the test statistic is continuous, then the <span class='clFormula'>$p$</span>
-value is uniformly distributed between 0 and 1 if the null hypothesis is true. The <span class='clFormula'>$p$</span>
-value is the probability of obtaining a test statistic at least as extreme as the one that was actually observed, assuming that the null hypothesis is true. One often "rejects the null hypothesis" when the <span class='clFormula'>$p$</span>
-value is less than the predetermined significance level, which is often 0.05 or 0.01, indicating that the observed result would be highly unlikely under the null hypothesis. Many common statistical tests, such as chi-squared tests or Student's <span class='clFormula'>$t$</span>
-test, produce test statistics which can be interpreted using <span class='clFormula'>$p$</span>
-values.</p>

<h3>Sampling from a Uniform Distribution</h3>

<p>There are many applications in which it is useful to run simulation experiments. Many programming languages have the ability to generate pseudo-random numbers which are effectively distributed according to the uniform distribution.</p>
<p>If <span class='clFormula'>$u$</span>
is a value sampled from the standard uniform distribution, then the value <span class='clFormula'>$a+(b-a)u$</span>
follows the uniform distribution parametrized by <span class='clFormula'>$a$</span>
and <span class='clFormula'>$b$</span>
.</p>

<h3>Sampling from an Arbitrary Distribution</h3>

<p>The uniform distribution is useful for sampling from arbitrary distributions. A general method is the inverse transform sampling method, which uses the cumulative distribution function (CDF) of the target random variable. This method is very useful in theoretical work. Since simulations using this method require inverting the CDF of the target variable, alternative methods have been devised for the cases where the CDF is not known in closed form. One such method is rejection sampling.</p>
<p>The normal distribution is an important example where the inverse transform method is not efficient. However, there is an exact method, the Box–Muller transformation, which uses the inverse transform to convert two independent uniform random variables into two independent normally distributed random variables.</p>

<h3>Example</h3>

<p>Imagine that the amount of time, in minutes, that a person must wait for a bus is uniformly distributed between 0 and 15 minutes. What is the probability that a person waits fewer than 12.5 minutes?</p>
<p>Let <span class='clFormula'>$X$</span>
 be the number of minutes a person must wait for a bus. <span class='clFormula'>$a=0$</span>
and <span class='clFormula'>$b=15$</span>
. <span class='clFormula'>$x \sim U(0, 15)$</span>
. The probability density function is written as:</p>
<p><span class='clFormula'>$f(x) = \frac{1}{15} - 0 = \frac{1}{15}$</span>
 for <span class='clFormula'>$0 \leq x \leq 15$</span></p>
<p>We want to find <span class='clFormula'>$P(x&lt;12.5)$</span>
.</p>
<p>The probability a person waits less than 12.5 minutes is 0.8333.</p>



<a href='../images/18395.jpeg'><img class='clImageThumb' src='../images/18395.jpeg' alt='Catching a Bus'></a>

<div><b><i>Catching a Bus</i></b></div>
<p><i>The Uniform Distribution can be used to calculate probability problems such as the probability of waiting for a bus for a certain amount of time.</i></p>

<h3 id='concept_189'>10.1.3. The Exponential Distribution</h3>

<blockquote>The exponential distribution is a family of continuous probability distributions that describe the time between events in a Poisson process.</blockquote>

<h4>Learning Objective</h4>

<p>Apply exponential distribution in describing time for a continuous process</p>

<h4>Key Points</h4>

<ul>
<li>The exponential distribution is often concerned with the amount of time until some specific event occurs.</li>
<li>Exponential variables can also be used to model situations where certain events occur with a constant probability per unit length, such as the distance between mutations on a DNA strand.</li>
<li>Values for an exponential random variable occur in such a way that there are fewer large values and more small values.</li>
<li>An important property of the exponential distribution is that it is memoryless.</li>
</ul>

<h4>Key Terms</h4>

<dl>
<dt>Erlang distribution</dt>
<dd>The distribution of the sum of several independent exponentially distributed variables.</dd>
<dt>Poisson process</dt>
<dd>A stochastic process in which events occur continuously and independently of one another.</dd>
</dl>

<p>The exponential distribution is a family of continuous probability distributions. It describes the time between events in a Poisson process (the process in which events occur continuously and independently at a constant average rate).</p>
<p>The exponential distribution is often concerned with the amount of time until some specific event occurs. For example, the amount of time (beginning now) until an earthquake occurs has an exponential distribution. Other examples include the length (in minutes) of long distance business telephone calls and the amount of time (in months) that a car battery lasts. It could also be shown that the value of the coins in your pocket or purse follows (approximately) an exponential distribution.</p>
<p>Values for an exponential random variable occur in such a way that there are fewer large values and more small values. For example, the amount of money customers spend in one trip to the supermarket follows an exponential distribution. There are more people that spend less money and fewer people that spend large amounts of money.</p>

<h3>Properties of the Exponential Distribution</h3>

<p>The mean or expected value of an exponentially distributed random variable <span class='clFormula'>$X$</span><em>,</em> with rate parameter <span class='clFormula'>$\lambda$</span><em>,</em> is given by the formula:</p>

<div class='clFormula'>$\displaystyle E[X] = \frac{1}{\lambda}$</div>

<p>Example: If you receive phone calls at an average rate of 2 per hour, you can expect to wait approximately thirty minutes for every call.</p>
<p>The variance of <span class='clFormula'>$X$</span>
is given by the formula:</p>

<div class='clFormula'>$\displaystyle \text{Var}[X] = \frac{1}{\lambda^2}$</div>

<p>In our example, the rate at which you receive phone calls will have a variance of 15 minutes.</p>
<p>Another important property of the exponential distribution is that it is memoryless. This means that if a random variable <span class='clFormula'>$T$</span>
is exponentially distributed, its conditional probability obeys the formula:</p>
<p><span class='clFormula'>$P(T&gt;s+t \ | \ T&gt;s) = P(T&gt;t)$</span>
 for all <span class='clFormula'>$s, t \geq 0$</span></p>
<p>The conditional probability that we need to wait, for example, more than another 10 seconds before the first arrival, given that the first arrival has not yet happened after 30 seconds, is equal to the initial probability that we need to wait more than 10 seconds for the first arrival. So, if we waited for 30 seconds and the first arrival didn't happen (<span class='clFormula'>$T&gt;30$</span>
), the probability that we'll need to wait another 10 seconds for the first arrival (<span class='clFormula'>$T&gt;(30+10)$</span>
) is the same as the initial probability that we need to wait more than 10 seconds for the first arrival (<span class='clFormula'>$T&gt;10$</span>
). The fact that  <span class='clFormula'>$P(T&gt;40 \ | \ T&gt;30) = P(T&gt;10)$</span>
 does not mean that the events <span class='clFormula'>$T&gt;40$</span>
and <span class='clFormula'>$T&gt;30$</span>
are independent.</p>

<h3>Applications of the Exponential Distribution</h3>

<p>The exponential distribution describes the time for a continuous process to change state. In real-world scenarios, the assumption of a constant rate (or probability per unit time) is rarely satisfied. For example, the rate of incoming phone calls differs according to the time of day. But if we focus on a time interval during which the rate is roughly constant, such as from 2 to 4 p.m. during work days, the exponential distribution can be used as a good approximate model for the time until the next phone call arrives. Similar caveats apply to the following examples which yield approximately exponentially distributed variables:</p>
<ul>
<li>the time until a radioactive particle decays, or the time between clicks of a geiger counter</li>
<li>the time until default (on payment to company debt holders) in reduced form credit risk modeling</li>
</ul>
<p>Exponential variables can also be used to model situations where certain events occur with a constant probability per unit length, such as the distance between mutations on a DNA strand, or between roadkills on a given road.</p>
<p>In queuing theory, the service times of agents in a system (e.g. how long it takes for a bank teller to serve a customer) are often modeled as exponentially distributed variables. The length of a process that can be thought of as a sequence of several independent tasks is better modeled by a variable following the Erlang distribution (which is the distribution of the sum of several independent exponentially distributed variables).</p>
<p>Reliability engineering also makes extensive use of the exponential distribution. Because of the memoryless property of this distribution, it is well-suited to model the constant hazard rate portion of the bathtub curve used in reliability theory. It is also very convenient because it is so easy to add failure rates in a reliability model. The exponential distribution is, however, not appropriate to model the overall lifetime of organisms or technical devices because the "failure rates" here are not constant: more failures occur for very young and for very old systems.</p>
<p>In hydrology, the exponential distribution is used to analyze extreme values of such variables as monthly and annual maximum values of daily rainfall and river discharge volumes.</p>

<h3 id='concept_190'>10.1.4. The Normal Distribution</h3>

<blockquote>The normal distribution is symmetric with scores more concentrated in the middle than in the tails.</blockquote>

<h4>Learning Objective</h4>

<p>Recognize the normal distribution from its characteristics</p>

<h4>Key Points</h4>

<ul>
<li>Physical quantities that are expected to be the sum of many independent processes (such as measurement errors) often have a distribution very close to normal.</li>
<li>The simplest case of normal distribution, known as the Standard Normal Distribution, has expected value zero and variance one.</li>
<li>If the mean and standard deviation are known, then one essentially knows as much as if he or she had access to every point in the data set.</li>
<li>The empirical rule is a handy quick estimate of the spread of the data given the mean and standard deviation of a data set that follows normal distribution.</li>
<li>The normal distribution is the most used statistical distribution, since normality arises naturally in many physical, biological, and social measurement situations.</li>
</ul>

<h4>Key Terms</h4>

<dl>
<dt>empirical rule</dt>
<dd>That a normal distribution has 68% of its observations within one standard deviation of the mean, 95% within two, and 99.7% within three.</dd>
<dt>entropy</dt>
<dd>A measure which quantifies the expected value of the information contained in a message.</dd>
<dt>cumulant</dt>
<dd>Any of a set of parameters of a one-dimensional probability distribution of a certain form.</dd>
</dl>

<p>Normal distributions are a family of distributions all having the same general shape. They are symmetric, with scores more concentrated in the middle than in the tails. Normal distributions are sometimes described as bell shaped.</p>
<p>The normal distribution is a continuous probability distribution, defined by the formula:</p>

<div class='clFormula'>$\displaystyle f(x) = \frac{1}{\sigma\sqrt{2\pi}}e^{\frac{(x-\mu)^2}{2\sigma^2}}$</div>

<p>The parameter <span class='clFormula'>$\mu$</span>
in this formula is the mean or expectation of the distribution (and also its median and mode). The parameter <span class='clFormula'>$\sigma$</span>
is its standard deviation; its variance is therefore <span class='clFormula'>$\sigma^2$</span>
. If <span class='clFormula'>$\mu = 0$</span>
and <span class='clFormula'>$\sigma = 1$</span>
, the distribution is called the standard normal distribution or the unit normal distribution, and a random variable with that distribution is a standard normal deviate.</p>
<p>Normal distributions are extremely important in statistics, and are often used in the natural and social sciences for real-valued random variables whose distributions are not known. One reason for their popularity is the central limit theorem, which states that (under mild conditions) the mean of a large number of random variables independently drawn from the same distribution is distributed approximately normally, irrespective of the form of the original distribution. Thus, physical quantities expected to be the sum of many independent processes (such as measurement errors) often have a distribution very close to normal. Another reason is that a large number of results and methods can be derived analytically, in explicit form, when the relevant variables are normally distributed.</p>
<p>The normal distribution is the only absolutely continuous distribution whose cumulants, other than the mean and variance, are all zero. It is also the continuous distribution with the maximum entropy for a given mean and variance.</p>

<h3>Standard Normal Distribution</h3>

<p>The simplest case of normal distribution, known as the Standard Normal Distribution, has expected value zero and variance one. This is written as N (0, 1), and is described by this probability density function:</p>

<div class='clFormula'>$\displaystyle \phi(x) = \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}x^2}$</div>

<p>The <span class='clFormula'>$\frac { 1 }{ \sqrt { 2\pi } }$</span>
factor in this expression ensures that the total area under the curve <span class='clFormula'>$\phi(x)$</span>
is equal to one. The <span class='clFormula'>$\frac{1}{2}$</span>
in the exponent ensures that the distribution has unit variance (and therefore also unit standard deviation). This function is symmetric around <span class='clFormula'>$x=0$</span>
, where it attains its maximum value <span class='clFormula'>$\frac { 1 }{ \sqrt { 2\pi } }$</span>
; and has inflection points at <span class='clFormula'>$+1$</span>
and <span class='clFormula'>$-1$</span>
.</p>

<h3>Characteristics of the Normal Distribution</h3>

<ul>
<li>It is a continuous distribution.</li>
<li>It is symmetrical about the mean. Each half of the distribution is a mirror image of the other half.</li>
<li>It is asymptotic to the horizontal axis.</li>
<li>It is unimodal.</li>
<li>The area under the curve is 1.</li>
</ul>
<p>The normal distribution carries with it assumptions and can be completely specified by two parameters: the mean and the standard deviation. This is written as <span class='clFormula'>$N(0, 1)$</span>
. If the mean and standard deviation are known, then one essentially knows as much as if he or she had access to every point in the data set.</p>
<p>The empirical rule is a handy quick estimate of the spread of the data given the mean and standard deviation of a data set that follows normal distribution. It states that:</p>
<ul>
<li>68% of the data will fall within 1 standard deviation of the mean.</li>
<li>95% of the data will fall within 2 standard deviations of the mean.</li>
<li>Almost all (99.7% ) of the data will fall within 3 standard deviations of the mean.</li>
</ul>
<p>The <em>strengths of the normal distribution</em> are that:</p>
<ul>
<li>it is probably the most widely known and used of all distributions,</li>
<li>it has infinitely divisible probability distributions, and</li>
<li>it has strictly stable probability distributions.</li>
</ul>
<p>The weakness of normal distributions is for reliability calculations. In this case, using the normal distribution starts at negative infinity. This case is able to result in negative values for some of the results.</p>

<h3>Importance and Application</h3>

<ul>
<li>Many things are normally distributed, or very close to it. For example, height and intelligence are approximately normally distributed.</li>
<li>The normal distribution is easy to work with mathematically. In many practical cases, the methods developed using normal theory work quite well even when the distribution is not normal.</li>
<li>There is a very strong connection between the size of a sample <span class='clFormula'>$N$</span>
and the extent to which a sampling distribution approaches the normal form. Many sampling distributions based on a large <span class='clFormula'>$N$</span>
can be approximated by the normal distribution even though the population distribution itself is not normal.</li>
<li>The normal distribution is the most used statistical distribution, since normality arises naturally in many physical, biological, and social measurement situations.</li>
</ul>
<p>In addition, normality is important in statistical inference. The normal distribution has applications in many areas of business administration. For example:</p>
<ul>
<li>Modern portfolio theory commonly assumes that the returns of a diversified asset portfolio follow a normal distribution.</li>
<li>In human resource management, employee performance sometimes is considered to be normally distributed.</li>
</ul><h3 id='concept_191'>10.1.5. Graphing the Normal Distribution</h3>

<blockquote>The graph of a normal distribution is a bell curve.</blockquote>

<h4>Learning Objective</h4>

<p>Evaluate a bell curve in order to picture the value of the standard deviation in a distribution</p>

<h4>Key Points</h4>

<ul>
<li>The mean of a normal distribution determines the height of a bell curve.</li>
<li>The standard deviation of a normal distribution determines the width or spread of a bell curve.</li>
<li>The larger the standard deviation, the wider the graph.</li>
<li>Percentiles represent the area under the normal curve, increasing from left to right.</li>
</ul>

<h4>Key Terms</h4>

<dl>
<dt>empirical rule</dt>
<dd>That a normal distribution has 68% of its observations within one standard deviation of the mean, 95% within two, and 99.7% within three.</dd>
<dt>bell curve</dt>
<dd>In mathematics, the bell-shaped curve that is typical of the normal distribution.</dd>
<dt>real number</dt>
<dd>An element of the set of real numbers; the set of real numbers include the rational numbers and the irrational numbers, but not all complex numbers.</dd>
</dl>

<p>The graph of a normal distribution is a bell curve, as shown below.</p>



<a href='../images/18101.gif'><img class='clImageThumb' src='../images/18101.gif' alt='The Bell Curve'></a>

<div><b><i>The Bell Curve</i></b></div>
<p><i>The graph of a normal distribution is known as a bell curve.</i></p>

<p>The properties of the bell curve are as follows.</p>
<ul>
<li>It is perfectly symmetrical.</li>
<li>It is unimodal (has a single mode).</li>
<li>Its domain is all real numbers.</li>
<li>The area under the curve is 1.</li>
</ul>
<p>Different values of the mean and standard deviation determine the density factor. Mean specifically determines the height of a bell curve, and standard deviation relates to the width or spread of the graph. The height of the graph at any <span class='clFormula'>$x$</span>
value can be found through the equation:</p>

<div class='clFormula'>$\displaystyle \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2}$</div>

<p>In order to picture the value of the standard deviation of a normal distribution and it's relation to the width or spread of a bell curve, consider the following graphs. Out of these two graphs, graph 1 and graph 2, which one represents a set of data with a larger standard deviation?</p>



<a href='../images/18104.gif'><img class='clImageThumb' src='../images/18104.gif' alt='Graph 1'></a>

<div><b><i>Graph 1</i></b></div>
<p><i>Bell curve visualizing a normal distribution with a relatively small standard deviation.</i></p>



<a href='../images/18103.gif'><img class='clImageThumb' src='../images/18103.gif' alt='Graph 2'></a>

<div><b><i>Graph 2</i></b></div>
<p><i>Bell curve visualizing a normal distribution with a relatively large standard deviation.</i></p>

<p>The correct answer is graph 2. The larger the standard deviation, the wider the graph. The smaller it is, the narrower the graph.</p>

<h3>Percentiles and the Normal Curve</h3>

<p>Percentiles represent the area under the normal curve, increasing from left to right. Each standard deviation represents a fixed percentile, and follows the empirical rule. Thus, rounding to two decimal places, <span class='clFormula'>$-3$</span>
is the 0.13<sup>th</sup> percentile, <span class='clFormula'>$-2$</span>
the 2.28<sup>th</sup> percentile, <span class='clFormula'>$-1$</span>
the 15.87<sup>th</sup> percentile, 0 the 50<sup>th</sup> percentile (both the mean and median of the distribution), <span class='clFormula'>$+1$</span>
the 84.13<sup>th</sup> percentile, <span class='clFormula'>$+2$</span>
the 97.72<sup>nd</sup> percentile, and <span class='clFormula'>$+3$</span>
the 99.87<sup>th</sup> percentile. Note that the 0<sup>th</sup> percentile falls at negative infinity and the 100<sup>th</sup> percentile at positive infinity.</p>

<h3 id='concept_192'>10.1.6. The Standard Normal Curve</h3>

<blockquote>The standard normal distribution is a normal distribution with a mean of 0 and a standard deviation of 1.</blockquote>

<h4>Learning Objective</h4>

<p>Explain how to derive standard normal distribution given a data set</p>

<h4>Key Points</h4>

<ul>
<li>The random variable of a standard normal distribution is denoted by <span class='clFormula'>$Z$</span>
, instead of <span class='clFormula'>$X$</span>
.</li>
<li>Unfortunately, in most cases in which the normal distribution plays a role, the mean is not 0 and the standard deviation is not 1.</li>
<li>Fortunately, one can transform any normal distribution with a certain mean <span class='clFormula'>$\mu$</span>
and standard deviation <span class='clFormula'>$\sigma$</span>
into a standard normal distribution, by the <span class='clFormula'>$z$</span>
-score conversion formula.</li>
<li>Of importance is that calculating <span class='clFormula'>$z$</span>
requires the population mean and the population standard deviation, not the sample mean or sample deviation.</li>
</ul>

<h4>Key Terms</h4>

<dl>
<dt>z-score</dt>
<dd>The standardized value of observation <equation>$x$</equation> from a distribution that has mean <equation>$\mu$</equation> and standard deviation <equation>$\sigma$</equation>.</dd>
<dt>standard normal distribution</dt>
<dd>The normal distribution with a mean of zero and a standard deviation of one.</dd>
</dl>

<p>If the mean (<span class='clFormula'>$\mu$</span>
) and standard deviation (<span class='clFormula'>$\sigma$</span>
) of a normal distribution are 0 and 1, respectively, then we say that the random variable follows a standard normal distribution. This type of random variable is often denoted by <span class='clFormula'>$Z$</span>
, instead of <span class='clFormula'>$X$</span>
.</p>
<p>The area above the <span class='clFormula'>$x$</span>
-axis and under the curve must equal one, with the area under the curve representing the probability. For example, <span class='clFormula'>$P(-2&lt;X&lt;2)$</span>
is the area under the curve between <span class='clFormula'>$x=-2$</span>
and <span class='clFormula'>$x=2$</span>
. Since the standard deviation is 1, this represents the probability that a normal distribution is between 2 standard deviations away from the mean. From the empirical rule, we know that this value is 0.95.</p>

<h3>Standardization</h3>

<p>Unfortunately, in most cases in which the normal distribution plays a role, the mean is not 0 and the standard deviation is not 1. Luckily, one can transform any normal distribution with a certain mean <span class='clFormula'>$\mu$</span>
and standard deviation <span class='clFormula'>$\sigma$</span>
into a standard normal distribution, by the <span class='clFormula'>$z$</span>
-score conversion formula:</p>

<div class='clFormula'>$\displaystyle z=\frac { x-\mu }{ \sigma }$</div>

<p>Therefore, a <span class='clFormula'>$z$</span>
-score is the standardized value of observation <span class='clFormula'>$x$</span>
from a distribution that has mean <span class='clFormula'>$\mu$</span>
and standard deviation <span class='clFormula'>$\sigma$</span>
(how many standard deviations you are away from zero). The <span class='clFormula'>$z$</span>
-score gets its name because of the denomination of the standard normal distribution as the "<span class='clFormula'>$Z$</span>
" distribution. It can be said to provide an assessment of how off-target a process is operating.</p>
<p>A key point is that calculating <span class='clFormula'>$z$</span>
requires the population mean and the population standard deviation, not the sample mean or sample deviation. It requires knowing the population parameters, not the statistics of a sample drawn from the population of interest. However, knowing the true standard deviation of a population is often unrealistic except in cases such as standardized testing, where the entire population is measured. In cases where it is impossible to measure every member of a population, the standard deviation may be estimated using a random sample.</p>

<h3>Example</h3>

<p>Assuming that the height of women in the US is normally distributed with a mean of 64 inches and a standard deviation of 2.5 inches, find the following:</p>

<ol>
<li>The probability that a randomly selected woman is taller than 70.4 inches (5 foot 10.4 inches).</li>
<li>The probability that a randomly selected woman is between 60.3 and 65 inches tall.</li>
</ol>
<p><em>Part one</em>: Since the height of women follows a normal distribution but not a standard normal, we first need to standardize. Since <span class='clFormula'>$x=70.4 \ \text{inches}$</span>
, <span class='clFormula'>$\mu=64 \ \text{inches}$</span>
and <span class='clFormula'>$\sigma = 2.5 \ \text{inches}$</span>
, we need to calculate <span class='clFormula'>$z$</span>
:</p>

<div class='clFormula'>$\displaystyle z=\frac { 70.4-64 }{ 2.5 } =\frac { 6.4 }{ 2.5 } =2.56$</div>

<p>Therefore, the probability <span class='clFormula'>$P(X&gt;70.4)$</span>
is equal to <span class='clFormula'>$P(Z&gt;2.56)$</span>
, where <span class='clFormula'>$X$</span>
is the normally distributed height with mean <span class='clFormula'>$\mu=64 \ \text{inches}$</span>
and standard deviation <span class='clFormula'>$\sigma = 2.5 \ \text{inches}$</span>
 (<span class='clFormula'>$\{X \sim N(64, 2.5)\}$</span>
, for short), and <span class='clFormula'>$Z$</span>
is a standard normal distribution <span class='clFormula'>$\{Z \sim N(0, 1)\}$</span>
.</p>
<p>The next step requires that we use what is known as the <span class='clFormula'>$z$</span>
-score table to calculate probabilities for the standard normal distribution. This table can be seen below.</p>



<a href='../images/18107.jpeg'><img class='clImageThumb' src='../images/18107.jpeg' alt='-table'></a>

<div><b><i>
<span class='clFormula'>$z$</span>
-table</i></b></div>
<p><i>The <span class='clFormula'>$z$</span>
-score table is used to calculate probabilities for the standard normal distribution.</i></p>

<p>From the table, we learn that:</p>

<div class='clFormula'>$P(X&gt;70.4)=P(Z&gt;2.56)$</div>


<div class='clFormula'>$\qquad \qquad \ \ \ =0.5-0.4948$</div>


<div class='clFormula'>$\qquad \qquad \ \ \ = 0.0012$</div>

<p><em>Part two</em>: For the second problem we have two values of <span class='clFormula'>$x$</span>
to standarize: <span class='clFormula'>$x_1 = 60.3$</span>
and <span class='clFormula'>$x_2 = 65$</span>
. Standardizing these values we obtain:</p>
<p><span class='clFormula'>$z_1 = -1.48$</span>
and <span class='clFormula'>$z_2 = 0.40$</span>
.</p>
<p>Notice that the first value is negative, which means that it is below the mean. Therefore:</p>

<div class='clFormula'>$P(60.3&lt;X&lt;65) = P(-1.48&lt;Z&lt;0.40)$</div>


<div class='clFormula'>$\qquad \qquad \qquad \quad = P(Z&lt;0.40)-P(Z&lt;-1.48)$</div>


<div class='clFormula'>$\qquad \qquad \qquad \quad = (0.5+0.1554)-(0.5-0694)$</div>


<div class='clFormula'>$\qquad \qquad \qquad \quad  = 0.6554-0.0694$</div>


<div class='clFormula'>$\qquad \qquad \qquad \quad = 0.5860$</div>


<h3 id='concept_193'>10.1.7. Finding the Area Under the Normal Curve</h3>

<blockquote>To calculate the probability that a variable is within a range in the normal distribution, we have to find the area under the normal curve.</blockquote>

<h4>Learning Objective</h4>

<p>Interpret a <equation>$z$</equation>-score table to calculate the probability that a variable is within range in a normal distribution</p>

<h4>Key Points</h4>

<ul>
<li>To calculate the area under a normal curve, we use a <span class='clFormula'>$z$</span>
-score table.</li>
<li>In a <span class='clFormula'>$z$</span>
-score table, the left most column tells you how many standard deviations above the the mean to 1 decimal place, the top row gives the second decimal place, and the intersection of a row and column gives the probability.</li>
<li>For example, if we want to know the probability that a variable is no more than 0.51 standard deviations above the mean, we find select the 6th row down (corresponding to 0.5) and the 2nd column (corresponding to 0.01).</li>
</ul>

<h4>Key Term</h4>

<dl>
<dt>z-score</dt>
<dd>The standardized value of observation <equation>$x$</equation> from a distribution that has mean <equation>$\mu$</equation> and standard deviation <equation>$\sigma$</equation>.</dd>
</dl>

<p>To calculate the probability that a variable is within a range in the normal distribution, we have to find the area under the normal curve. In order to do this, we use a <span class='clFormula'>$z$</span>
-score table. (Same as in the process of standardization discussed in the previous section).</p>



<a href='../images/18108.jpeg'><img class='clImageThumb' src='../images/18108.jpeg' alt='Areas Under the Normal Curve'></a>

<div><b><i>Areas Under the Normal Curve</i></b></div>
<p><i>This table gives the cumulative probability up to the standardized normal value <span class='clFormula'>$z$</span>
.</i></p>

<p>These tables can seem a bit daunting; however, the key is knowing how to read them.</p>
<ul>
<li>The left most column tells you how many standard deviations above the the mean to 1 decimal place.</li>
<li>The top row gives the second decimal place.</li>
<li>The intersection of a row and column gives the probability.</li>
</ul>
<p>For example, if we want to know the probability that a variable is no more than 0.51 standard deviations above the mean, we find select the 6<sup>th</sup> row down (corresponding to 0.5) and the 2<sup>nd</sup> column (corresponding to 0.01). The intersection of the 6<sup>th</sup> row and 2<sup>nd</sup> column is 0.6950. This tells us that there is a 69.50% percent chance that a variable is less than 0.51 sigmas above the mean.</p>
<p>Notice that for 0.00 standard deviations, the probability is 0.5000. This shows us that there is equal probability of being above or below the mean.</p>
<p>Consider the following as a simple example: find <span class='clFormula'>$P(Z\leq 1.5)$</span><em>.</em></p>
<p>This problem essentially asks what is the probability that a variable is less than 1.5 standard deviations above the mean. On the table of values, find the row that corresponds to 1.5 and the column that corresponds to 0.00. This gives us a probability of 0.933.</p>
<p>The following is another simple example: find <span class='clFormula'>$P(Z\geq 1.17)$</span><em>.</em></p>
<p>This problem essentially asks what is the probability that a variable is MORE than 1.17 standard deviation above the mean. On the table of values, find the row that corresponds to 1.1 and the column that corresponds to 0.07. This gives us a probability of 0.8790. However, this is the probability that the value is less than 1.17 sigmas above the mean. Since all the probabilities must sum to 1:</p>

<div class='clFormula'>$P(Z&gt;1.17) = 1-P(Z&lt;1.17) = 0.121$</div>

<p>As a final example: find <span class='clFormula'>$P(-1.16\leq Z\leq 1.32)$</span>
.</p>
<p>This example is a bit tougher. The problem can be rewritten in the form below.</p>

<div class='clFormula'>$P(-1.16\leq Z\leq 1.32) = P(Z\leq 1.32) - P(Z\leq -1.16)$</div>

<p>The difficulty arrises from the fact that our table of values does not allow us to directly calculate <span class='clFormula'>$P(Z\leq -1.16)$</span>
. However, we can use the symmetry of the distribution, as follows:</p>

<div class='clFormula'>$P(Z\leq -1.16) = 1-P(Z\leq 1.16) = 0.1230$</div>

<p>So, we can say that:</p>

<div class='clFormula'>$P(-1.16\leq Z \leq 1.32) = 0.9066 - 0.1230 = 0.7836$</div>

<h2 id='section_40'>10.2. Normal Approximation</h2>

<h3 id='concept_194'>10.2.1. The Normal Approximation to the Binomial Distribution</h3>

<blockquote>The process of using the normal curve to estimate the shape of the binomial distribution is known as normal approximation.</blockquote>

<h4>Learning Objective</h4>

<p>Explain the origins of central limit theorem for binomial distributions</p>

<h4>Key Points</h4>

<ul>
<li>Originally, to solve a problem such as the chance of obtaining 60 heads in 100 coin flips, one had to compute the probability of 60 heads, then the probability of 61 heads, 62 heads, etc, and add up all these probabilities.</li>
<li>Abraham de Moivre noted that when the number of events (coin flips) increased, the shape of the binomial distribution approached a very smooth curve.</li>
<li>Therefore, de Moivre reasoned that if he could find a mathematical expression for this curve, he would be able to solve problems such as finding the probability of 60 or more heads out of 100 coin flips much more easily.</li>
<li>This is exactly what he did, and the curve he discovered is now called the normal curve.</li>
</ul>

<h4>Key Terms</h4>

<dl>
<dt>normal approximation</dt>
<dd>The process of using the normal curve to estimate the shape of the distribution of a data set.</dd>
<dt>central limit theorem</dt>
<dd>The theorem that states: If the sum of independent identically distributed random variables has a finite variance, then it will be (approximately) normally distributed.</dd>
</dl>

<p>The binomial distribution can be used to solve problems such as, "If a fair coin is flipped 100 times, what is the probability of getting 60 or more heads?" The probability of exactly <span class='clFormula'>$x$</span>
heads out of <span class='clFormula'>$N$</span>
flips is computed using the formula:</p>

<div class='clFormula'>$\displaystyle P\left( x \right) =\frac { N! }{ x!\left( N-x \right) ! } { \pi }^{ x }{ \left( 1-\pi \right) }^{ N-x }$</div>

<p>where <span class='clFormula'>$x$</span>
is the number of heads (60), <span class='clFormula'>$N$</span>
is the number of flips (100), and <span class='clFormula'>$\pi$</span>
is the probability of a head (0.5). Therefore, to solve this problem, you compute the probability of 60 heads, then the probability of 61 heads, 62 heads, etc, and add up all these probabilities.</p>
<p>Abraham de Moivre, an 18<sup>th</sup> century statistician and consultant to gamblers, was often called upon to make these lengthy computations. de Moivre noted that when the number of events (coin flips) increased, the shape of the binomial distribution approached a very smooth curve. Therefore, de Moivre reasoned that if he could find a mathematical expression for this curve, he would be able to solve problems such as finding the probability of 60 or more heads out of 100 coin flips much more easily. This is exactly what he did, and the curve he discovered is now called the normal curve. The process of using this curve to estimate the shape of the binomial distribution is known as normal approximation.</p>



<a href='../images/18267.png'><img class='clImageThumb' src='../images/18267.png' alt='Normal Approximation'></a>

<div><b><i>Normal Approximation</i></b></div>
<p><i>The normal approximation to the binomial distribution for 12 coin flips. The smooth curve is the normal distribution. Note how well it approximates the binomial probabilities represented by the heights of the blue lines.</i></p>

<p>The importance of the normal curve stems primarily from the fact that the distribution of many natural phenomena are at least approximately normally distributed. One of the first applications of the normal distribution was to the analysis of errors of measurement made in astronomical observations, errors that occurred because of imperfect instruments and imperfect observers. Galileo in the 17<sup>th</sup> century noted that these errors were symmetric and that small errors occurred more frequently than large errors. This led to several hypothesized distributions of errors, but it was not until the early 19<sup>th</sup> century that it was discovered that these errors followed a normal distribution. Independently the mathematicians Adrian (in 1808) and Gauss (in 1809) developed the formula for the normal distribution and showed that errors were fit well by this distribution.</p>
<p>This same distribution had been discovered by Laplace in 1778—when he derived the extremely important central limit theorem. Laplace showed that even if a distribution is not normally distributed, the means of repeated samples from the distribution would be very nearly normal, and that the the larger the sample size, the closer the distribution would be to a normal distribution. Most statistical procedures for testing differences between means assume normal distributions. Because the distribution of means is very close to normal, these tests work well even if the distribution itself is only roughly normal.</p>

<h3 id='concept_195'>10.2.2. The Scope of the Normal Approximation</h3>

<blockquote>The scope of the normal approximation is dependent upon our sample size, becoming more accurate as the sample size grows.</blockquote>

<h4>Learning Objective</h4>

<p>Explain how central limit theorem is applied in normal approximation</p>

<h4>Key Points</h4>

<ul>
<li>The tool of normal approximation allows us to approximate the probabilities of random variables for which we don't know all of the values, or for a very large range of potential values that would be very difficult and time consuming to calculate.</li>
<li>The scope of the normal approximation follows with the statistical themes of the law of large numbers and central limit theorem.</li>
<li>According to the law of large numbers, the average of the results obtained from a large number of trials should be close to the expected value, and will tend to become closer as more trials are performed.</li>
<li>The central limit theorem (CLT) states that, given certain conditions, the mean of a sufficiently large number of independent random variables, each with a well-defined mean and well-defined variance, will be approximately normally distributed.</li>
</ul>

<h4>Key Terms</h4>

<dl>
<dt>law of large numbers</dt>
<dd>The statistical tendency toward a fixed ratio in the results when an experiment is repeated a large number of times.</dd>
<dt>central limit theorem</dt>
<dd>The theorem that states: If the sum of independent identically distributed random variables has a finite variance, then it will be (approximately) normally distributed.</dd>
<dt>normal approximation</dt>
<dd>The process of using the normal curve to estimate the shape of the distribution of a data set.</dd>
</dl>

<p>The tool of normal approximation allows us to approximate the probabilities of random variables for which we don't know all of the values, or for a very large range of potential values that would be very difficult and time consuming to calculate. We do this by converting the range of values into standardized units and finding the area under the normal curve. A problem arises when there are a limited number of samples, or draws in the case of data "drawn from a box." A probability histogram of such a set may not resemble the normal curve, and therefore the normal curve will not accurately represent the expected values of the random variables. In other words, the scope of the normal approximation is dependent upon our sample size, becoming more accurate as the sample size grows. This characteristic follows with the statistical themes of the law of large numbers and central limit theorem (reviewed below).</p>

<h3>Law of Large Numbers</h3>

<p>The law of large numbers (LLN) is a theorem that describes the result of performing the same experiment a large number of times. According to the law, the average of the results obtained from a large number of trials should be close to the expected value, and will tend to become closer as more trials are performed.</p>
<p>The law of large numbers is important because it "guarantees" stable long-term results for the averages of random events. For example, while a casino may lose money in a single spin of the roulette wheel, its earnings will tend towards a predictable percentage over a large number of spins. Any winning streak by a player will eventually be overcome by the parameters of the game. It is important to remember that the LLN only applies (as the name indicates) when a large number of observations are considered. There is no principle that a small number of observations will coincide with the expected value or that a streak of one value will immediately be "balanced" by the others.</p>



<a href='../images/18277.svg'><img class='clImageThumb' src='../images/18277.svg' alt='Law of Large Numbers'></a>

<div><b><i>Law of Large Numbers</i></b></div>
<p><i>An illustration of the law of large numbers using a particular run of rolls of a single die. As the number of rolls in this run increases, the average of the values of all the results approaches 3.5. While different runs would show a different shape over a small number of throws (at the left), over a large number of rolls (to the right) they would be extremely similar.</i></p>

<h3>Central Limit Theorem</h3>

<p>The central limit theorem (CLT) states that, given certain conditions, the mean of a sufficiently large number of independent random variables, each with a well-defined mean and well-defined variance, will be approximately normally distributed. The central limit theorem has a number of variants. In its common form, the random variables must be identically distributed. In variants, convergence of the mean to the normal distribution also occurs for non-identical distributions, given that they comply with certain conditions.</p>
<p>More precisely, the central limit theorem states that as <span class='clFormula'>$n$</span>
gets larger, the distribution of the difference between the sample average <span class='clFormula'>$S_n$</span>
and its limit <span class='clFormula'>$\mu$</span>
, when multiplied by the factor:</p>
<p><span class='clFormula'>$\sqrt { n }$</span>
(that is <span class='clFormula'>$\sqrt { n } ({ S }_{ n }-\mu )$</span>
)</p>
<p>Approximates the normal distribution with mean 0 and variance <span class='clFormula'>$\sigma^2$</span>
. For large enough <span class='clFormula'>$n$</span>
, the distribution of <span class='clFormula'>$S_n$</span>
is close to the normal distribution with mean <span class='clFormula'>$\mu$</span>
and variance <span class='clFormula'>$\frac { { \sigma }^{ 2 } }{ n }$</span>
. The usefulness of the theorem is that the distribution of <span class='clFormula'>$\sqrt { n } ({ S }_{ n }-\mu )$</span>
approaches normality regardless of the shape of the distribution of the individual <span class='clFormula'>$X_i$</span>
's .</p>



<a href='../images/18278.png'><img class='clImageThumb' src='../images/18278.png' alt='Central Limit Theorem'></a>

<div><b><i>Central Limit Theorem</i></b></div>
<p><i>A distribution being "smoothed out" by summation, showing original density of distribution and three subsequent summations</i></p>

<h3 id='concept_196'>10.2.3. Calculating a Normal Approximation</h3>

<blockquote>In this atom, we provide an example on how to compute a normal approximation for a binomial distribution.</blockquote>

<h4>Learning Objective</h4>

<p>Demonstrate how to compute normal approximation for a binomial distribution</p>

<h4>Key Points</h4>

<ul>
<li>In our example, we have a fair coin and wish to know the probability that you would get 8 heads out of 10 flips.</li>
<li>The binomial distribution has a mean of <span class='clFormula'>$\mu = Np = 10\cdot 0.5 = 5$</span>
and a variance of <span class='clFormula'>$\sigma^2 = Np(1-p) = 10 \cdot 0.5\cdot 0.5 = 2.5$</span>
; therefore a standard deviation of 1.5811.</li>
<li>A total of 8 heads is 1.8973 standard deviations above the mean of the distribution.</li>
<li>Because the binomial distribution is discrete an the normal distribution is continuous, we round off and consider any value from 7.5 to 8.5 to represent an outcome of 8 heads.</li>
<li>Using this approach, we calculate the area under a normal curve (which will be the binomial probability) from 7.5 to 8.5 to be 0.044.</li>
</ul>

<h4>Key Terms</h4>

<dl>
<dt>binomial distribution</dt>
<dd>the discrete probability distribution of the number of successes in a sequence of <equation>$n$</equation> independent yes/no experiments, each of which yields success with probability <equation>$p$</equation></dd>
<dt>z-score</dt>
<dd>The standardized value of observation <equation>$x$</equation> from a distribution that has mean <equation>$\mu$</equation> and standard deviation <equation>$\sigma$</equation>.</dd>
</dl>

<p>The following is an example on how to compute a normal approximation for a binomial distribution.</p>
<p>Assume you have a fair coin and wish to know the probability that you would get 8 heads out of 10 flips. The binomial distribution has a mean of <span class='clFormula'>$\mu = Np = 10\cdot 0.5 = 5$</span>
 and a variance of <span class='clFormula'>$\sigma^2 = Np(1-p) = 10 \cdot 0.5\cdot 0.5 = 2.5$</span>
. The standard deviation is, therefore, 1.5811. A total of 8 heads is:</p>

<div class='clFormula'>$\displaystyle \frac { 8-5 }{ 1.5811 } =1.8973$</div>

<p>Standard deviations above the mean of the distribution. The question then is, "What is the probability of getting a value exactly 1.8973 standard deviations above the mean?" You may be surprised to learn that the answer is 0 (the probability of any one specific point is 0). The problem is that the binomial distribution is a discrete probablility distribution whereas the normal distribultion is a continuous distribution.</p>
<p>The solution is to round off and consider any value from 7.5 to 8.5 to represent an outcome of 8 heads. Using this approach, we calculate the area under a normal curve from 7.5 to 8.5. The area in green in the figure is an approximation of the probability of obtaining 8 heads.</p>



<a href='../images/18268.png'><img class='clImageThumb' src='../images/18268.png' alt='Normal Approximation'></a>

<div><b><i>Normal Approximation</i></b></div>
<p><i>Approximation for the probability of 8 heads with the normal distribution.</i></p>

<p>To calculate this area, first we compute the area below 8.5 and then subtract the area below 7.5. This can be done by finding <span class='clFormula'>$z$</span>
-scores and using the <span class='clFormula'>$z$</span>
-score table. Here, for the sake of ease, we have used an online normal area calculator. The results are shown in the following figures:</p>



<a href='../images/18270.png'><img class='clImageThumb' src='../images/18270.png' alt='Normal Area 2'></a>

<div><b><i>Normal Area 2</i></b></div>
<p><i>This graph shows the area below 7.5.</i></p>



<a href='../images/18269.png'><img class='clImageThumb' src='../images/18269.png' alt='Normal Area 1'></a>

<div><b><i>Normal Area 1</i></b></div>
<p><i>This graph shows the area below 8.5.</i></p>



<a href='../images/18478.jpeg'><img class='clImageThumb' src='../images/18478.jpeg' alt='-Score Table'></a>

<div><b><i>
<span class='clFormula'>$z$</span>
-Score Table</i></b></div>
<p><i>The <span class='clFormula'>$z$</span>
-score table is used to calculate probabilities for the standard normal distribution.</i></p>

<p>The differences between the areas is 0.044, which is the approximation of the binomial probability. For these parameters, the approximation is very accurate. If we did not have the normal area calculator, we could find the solution using a table of the standard normal distribution (a <span class='clFormula'>$z$</span>
-table) as follows:</p>

<ol>
<li>Find a <span class='clFormula'>$Z$</span>
score for 7.5 using the formula <span class='clFormula'>$Z=\frac { 7.5-5 }{ 1.5811 } =1.5811$</span></li>
<li>Find the area below a <span class='clFormula'>$Z$</span>
of <span class='clFormula'>$1.58=0.943$</span>
.</li>
<li>Find a <span class='clFormula'>$Z$</span>
score for 8.5 using the formula <span class='clFormula'>$Z=\frac { 8.5-5 }{ 1.5811 } =2.21$</span></li>
<li>Find the area below a <span class='clFormula'>$Z$</span>
of <span class='clFormula'>$2.21=0.987$</span>
.</li>
<li>Subtract the value in step 2 from the value in step 4 to get 0.044.</li>
</ol>
<p>The same logic applies when calculating the probability of a range of outcomes. For example, to calculate the probability of 8 to 10 flips, calculate the area from 7.5 to 10.5.</p>

<h3 id='concept_197'>10.2.4. Change of Scale</h3>

<blockquote>In order to consider a normal distribution or normal approximation, a standard scale or standard units is necessary.</blockquote>

<h4>Learning Objective</h4>

<p>Explain the significance of normalization of ratings and calculate this normalization</p>

<h4>Key Points</h4>

<ul>
<li>In the simplest cases, normalization of ratings means adjusting values measured on different scales to a notionally common scale, often prior to averaging.</li>
<li>In more complicated cases, normalization may refer to more sophisticated adjustments where the intention is to bring the entire probability distributions of adjusted values into alignment.</li>
<li>The standard score is a dimensionless quantity obtained by subtracting the population mean from an individual raw score and then dividing the difference by the population standard deviation.</li>
<li>A key point is that calculating <span class='clFormula'>$z$</span>
requires the population mean and the population standard deviation, not the sample mean or sample deviation.</li>
</ul>

<h4>Key Terms</h4>

<dl>
<dt>datum</dt>
<dd>A measurement of something on a scale understood by both the recorder (a person or device) and the reader (another person or device).</dd>
<dt>standard score</dt>
<dd>The number of standard deviations an observation or datum is above the mean.</dd>
<dt>normalization</dt>
<dd>The process of removing statistical error in repeated measured data.</dd>
</dl>

<p>In order to consider a normal distribution or normal approximation, a standard scale or standard units is necessary.</p>

<h3>Normalization</h3>

<p>In the simplest cases, normalization of ratings means adjusting values measured on different scales to a notionally common scale, often prior to averaging. In more complicated cases, normalization may refer to more sophisticated adjustments where the intention is to bring the entire probability distributions of adjusted values into alignment. In the case of normalization of scores in educational assessment, there may be an intention to align distributions to a normal distribution. A different approach to normalization of probability distributions is quantile normalization, where the quantiles of the different measures are brought into alignment.</p>
<p>Normalization can also refer to the creation of shifted and scaled versions of statistics, where the intention is that these normalized values allow the comparison of corresponding normalized values for different datasets. Some types of normalization involve only a rescaling, to arrive at values relative to some size variable.</p>

<h3>The Standard Score</h3>

<p>The standard score is the number of standard deviations an observation or datum is above the mean. Thus, a positive standard score represents a datum above the mean, while a negative standard score represents a datum below the mean. It is a dimensionless quantity obtained by subtracting the population mean from an individual raw score and then dividing the difference by the population standard deviation. This conversion process is called standardizing or normalizing.</p>
<p>Standard scores are also called <span class='clFormula'>$z$</span>
-values, <span class='clFormula'>$z$</span>
-scores, normal scores, and standardized variables. The use of "<span class='clFormula'>$Z$</span>
" is because the normal distribution is also known as the "<span class='clFormula'>$Z$</span>
distribution". They are most frequently used to compare a sample to a standard normal deviate (standard normal distribution, with <span class='clFormula'>$\mu = 0$</span>
and <span class='clFormula'>$\sigma = 1$</span>
).</p>
<p>The <span class='clFormula'>$z$</span>
-score is only defined if one knows the population parameters. If one only has a sample set, then the analogous computation with sample mean and sample standard deviation yields the Student's <span class='clFormula'>$t$</span>
-statistic.</p>
<p>The standard score of a raw score <span class='clFormula'>$x$</span>
is:</p>

<div class='clFormula'>$\displaystyle z=\frac { x-\mu }{ \sigma }$</div>

<p>Where <span class='clFormula'>$\mu$</span>
is the mean of the population, and is the standard deviation of the population. The absolute value of <span class='clFormula'>$z$</span>
represents the distance between the raw score and the population mean in units of the standard deviation. <span class='clFormula'>$z$</span>
is negative when the raw score is below the mean, positive when above.</p>
<p>A key point is that calculating <span class='clFormula'>$z$</span>
requires the population mean and the population standard deviation, not the sample mean or sample deviation. It requires knowing the population parameters, not the statistics of a sample drawn from the population of interest. However, knowing the true standard deviation of a population is often unrealistic except in cases such as standardized testing, where the entire population is measured. In cases where it is impossible to measure every member of a population, a random sample may be used.</p>
<p>The <span class='clFormula'>$Z$</span>
value measures the sigma distance of actual data from the average and provides an assessment of how off-target a process is operating.</p>



<a href='../images/18274.gif'><img class='clImageThumb' src='../images/18274.gif' alt='Normal Distribution and Scales'></a>

<div><b><i>Normal Distribution and Scales</i></b></div>
<p><i>Compares the various grading methods in a normal distribution. Includes: standard deviations, cumulative percentages, percentile equivalents, <span class='clFormula'>$Z$</span>
-scores, <span class='clFormula'>$T$</span>
-scores, and standard nine.</i></p>
<h2 id='section_41'>10.3. Measurement Error</h2>

<h3 id='concept_198'>10.3.1. Bias</h3>

<blockquote>Systematic, or biased, errors are errors which consistently yield results either higher or lower than the correct measurement.</blockquote>

<h4>Learning Objective</h4>

<p>Contrast random and systematic errors</p>

<h4>Key Points</h4>

<ul>
<li>Systematic errors are biases in measurement which lead to a situation wherein the mean of many separate measurements differs significantly from the actual value of the measured attribute in one direction.</li>
<li>A systematic error makes the measured value always smaller or larger than the true value, but not both. An experiment may involve more than one systematic error and these errors may nullify one another, but each alters the true value in one way only.</li>
<li>Accuracy (or validity) is a measure of the systematic error. If an experiment is accurate or valid, then the systematic error is very small.</li>
<li>Systematic errors include personal errors, instrumental errors, and method errors.</li>
</ul>

<h4>Key Terms</h4>

<dl>
<dt>systematic error</dt>
<dd>an error which consistently yields results either higher or lower than the correct measurement; accuracy error</dd>
<dt>random error</dt>
<dd>an error which is a combination of results both higher and lower than the desired measurement; precision error</dd>
<dt>Accuracy</dt>
<dd>the degree of closeness of measurements of a quantity to that quantity's actual (true) value</dd>
</dl>

<h3>Two Types of Errors</h3>

<p>While conducting measurements in experiments, there are generally two different types of errors: random (or chance) errors and systematic (or biased) errors.</p>
<p>Every measurement has an inherent uncertainty. We therefore need to give some indication of the reliability of measurements and the uncertainties of the results calculated from these measurements. To better understand the outcome of experimental data, an estimate of the size of the systematic errors compared to the random errors should be considered. Random errors are due to the precision of the equipment , and systematic errors are due to how well the equipment was used or how well the experiment was controlled .</p>



<a href='../images/18112.svg'><img class='clImageThumb' src='../images/18112.svg' alt='Low Accuracy, High Precision'></a>

<div><b><i>Low Accuracy, High Precision</i></b></div>
<p><i>This target shows an example of low accuracy (points are not close to center target) but high precision (points are close together). In this case, there is more systematic error than random error.</i></p>



<a href='../images/18110.svg'><img class='clImageThumb' src='../images/18110.svg' alt='High Accuracy, Low Precision'></a>

<div><b><i>High Accuracy, Low Precision</i></b></div>
<p><i>This target shows an example of high accuracy (points are all close to center target) but low precision (points are not close together). In this case, there is more random error than systematic error.</i></p>

<h3>Biased, or Systematic, Errors</h3>

<p>Systematic errors are biases in measurement which lead to a situation wherein the mean of many separate measurements differs significantly from the actual value of the measured attribute. All measurements are prone to systematic errors, often of several different types. Sources of systematic errors may be imperfect calibration of measurement instruments, changes in the environment which interfere with the measurement process, and imperfect methods of observation.</p>
<p>A systematic error makes the measured value always smaller <em>or</em> larger than the true value, but not both. An experiment may involve more than one systematic error and these errors may nullify one another, but each alters the true value in one way only. Accuracy (or validity) is a measure of the systematic error. If an experiment is accurate or valid, then the systematic error is very small. Accuracy is a measure of how well an experiment measures what it was trying to measure. This is difficult to evaluate unless you have an idea of the expected value (e.g. a text book value or a calculated value from a data book). Compare your experimental value to the literature value. If it is within the margin of error for the random errors, then it is most likely that the systematic errors are smaller than the random errors. If it is larger, then you need to determine where the errors have occurred. When an accepted value is available for a result determined by experiment, the percent error can be calculated.</p>
<p>For example, consider an experimenter taking a reading of the time period of a pendulum's full swing. If their stop-watch or timer starts with 1 second on the clock, then all of their results will be off by 1 second. If the experimenter repeats this experiment twenty times (starting at 1 second each time), then there will be a percentage error in the calculated average of their results; the final result will be slightly larger than the true period.</p>

<h3>Categories of Systematic Errors and How to Reduce Them</h3>


<ol>
<li>
<em>Personal Errors</em>: These errors are the result of ignorance, carelessness, prejudices, or physical limitations on the experimenter. This type of error can be greatly reduced if you are familiar with the experiment you are doing.</li>
<li>
<em>Instrumental Errors</em>: Instrumental errors are attributed to imperfections in the tools with which the analyst works. For example, volumetric equipment, such as burets, pipets, and volumetric flasks, frequently deliver or contain volumes slightly different from those indicated by their graduations. Calibration can eliminate this type of error.</li>
<li>
<em>Method Errors</em>: This type of error many times results when you do not consider how to control an experiment. For any experiment, ideally you should have only one manipulated (independent) variable. Many times this is very difficult to accomplish. The more variables you can control in an experiment, the fewer method errors you will have.</li>
</ol><h3 id='concept_199'>10.3.2. Chance Error</h3>

<blockquote>Random, or chance, errors are errors that are a combination of results both higher and lower than the desired measurement.</blockquote>

<h4>Learning Objective</h4>

<p>Explain how random errors occur within an experiment</p>

<h4>Key Points</h4>

<ul>
<li>A random error makes the measured value both smaller and larger than the true value; they are errors of precision.</li>
<li>Random errors occur by chance and cannot be avoided.</li>
<li>Random error is due to factors which we do not, or cannot, control.</li>
</ul>

<h4>Key Terms</h4>

<dl>
<dt>systematic error</dt>
<dd>an error which consistently yields results either higher or lower than the correct measurement; accuracy error</dd>
<dt>random error</dt>
<dd>an error which is a combination of results both higher and lower than the desired measurement; precision error</dd>
<dt>Precision</dt>
<dd>the ability of a measurement to be reproduced consistently</dd>
</dl>

<h3>Two Types of Errors</h3>

<p>While conducting measurements in experiments, there are generally two different types of errors: random (or chance) errors and systematic (or biased) errors.</p>
<p>Every measurement has an inherent uncertainty. We therefore need to give some indication of the reliability of measurements and the uncertainties of the results calculated from these measurements. To better understand the outcome of experimental data, an estimate of the size of the systematic errors compared to the random errors should be considered. Random errors are due to the precision of the equipment , and systematic errors are due to how well the equipment was used or how well the experiment was controlled .</p>



<a href='../images/18113.svg'><img class='clImageThumb' src='../images/18113.svg' alt='Low Accuracy, High Precision'></a>

<div><b><i>Low Accuracy, High Precision</i></b></div>
<p><i>This target shows an example of low accuracy (points are not close to center target) but high precision (points are close together). In this case, there is more systematic error than random error.</i></p>



<a href='../images/18111.svg'><img class='clImageThumb' src='../images/18111.svg' alt='High Accuracy, Low Precision'></a>

<div><b><i>High Accuracy, Low Precision</i></b></div>
<p><i>This target shows an example of high accuracy (points are all close to center target) but low precision (points are not close together). In this case, there is more random error than systematic error.</i></p>

<h3>Chance, or Random Errors</h3>

<p>A random error makes the measured value both smaller <em>and</em> larger than the true value; they are errors of precision. Chance alone determines if the value is smaller or larger. Reading the scales of a balance, graduated cylinder, thermometer, etc. produces random errors. In other words, you can weigh a dish on a balance and get a different answer each time simply due to random errors. They cannot be avoided; they are part of the measuring process. Uncertainties are measures of random errors. These are errors incurred as a result of making measurements on imperfect tools which can only have certain degree of precision.</p>
<p>Random error is due to factors which we cannot (or do not) control. It may be too expensive, or we may be too ignorant of these factors to control them each time we measure. It may even be that whatever we are trying to measure is changing in time or is fundamentally probabilistic. Random error often occurs when instruments are pushed to their limits. For example, it is common for digital balances to exhibit random error in their least significant digit. Three measurements of a single object might read something like 0.9111g, 0.9110g, and 0.9112g.</p>

<h3 id='concept_200'>10.3.3. Outliers</h3>

<blockquote>In statistics, an outlier is an observation that is numerically distant from the rest of the data.</blockquote>

<h4>Learning Objective</h4>

<p>Explain how to identify outliers in a distribution</p>

<h4>Key Points</h4>

<ul>
<li>Outliers can occur by chance, by human error, or by equipment malfunction. They may be indicative of a non-normal distribution, or they may just be natural deviations that occur in a large sample.</li>
<li>Unless it can be ascertained that the deviation is not significant, it is not wise to ignore the presence of outliers.</li>
<li>There is no rigid mathematical definition of what constitutes an outlier. Often, however, we use the rule of thumb that any point that is located further than two standard deviations above or below the best fit line is an outlier.</li>
</ul>

<h4>Key Terms</h4>

<dl>
<dt>interquartile range</dt>
<dd>The difference between the first and third quartiles; a robust measure of sample dispersion.</dd>
<dt>regression line</dt>
<dd>A smooth curve fitted to the set of paired data in regression analysis; for linear regression the curve is a straight line.</dd>
<dt>best fit line</dt>
<dd>A line on a graph showing the general direction that a group of points seem to be heading.</dd>
<dt>outlier</dt>
<dd>a value in a statistical sample which does not fit a pattern that describes most other data points; specifically, a value that lies 1.5 IQR beyond the upper or lower quartile</dd>
</dl>

<h3>Outliers</h3>

<p>In statistics, an outlier is an observation that is numerically distant from the rest of the data. Outliers can occur by chance in any distribution, but they are often indicative either of measurement error or that the population has a heavy-tailed distribution. In the former case, one wishes to discard them or use statistics that are robust to outliers, while in the latter case, they indicate that the distribution is skewed and that one should be very cautious in using tools or intuitions that assume a normal distribution.</p>
<p>When looking at regression lines that show where the data points fall, outliers are far away from the best fit line. They have large "errors," where the "error" or residual is the vertical distance from the line to the point.</p>
<p>Outliers need to be examined closely. Sometimes, for some reason or another, they should not be included in the analysis of the data. It is possible that an outlier is a result of erroneous data. Other times, an outlier may hold valuable information about the population under study and should remain included in the data. The key is to carefully examine what causes a data point to be an outlier.</p>

<h3>Identifying Outliers</h3>

<p>We could guess at outliers by looking at a graph of the scatterplot and best fit line. However, we would like some guideline as to how far away a point needs to be in order to be considered an outlier. As a rough rule of thumb, we can flag <em>any point that is located further than two standard deviations above or below the best fit line as an outlier,</em> as illustrated below. The standard deviation used is the standard deviation of the residuals or errors.</p>



<a href='../images/32455.png'><img class='clImageThumb' src='../images/32455.png' alt='Statistical outliers'></a>

<div><b><i>Statistical outliers</i></b></div>
<p><i>This graph shows a best-fit line (solid blue) to fit the data points, as well as two extra lines (dotted blue) that are two standard deviations above and below the best fit line. Highlighted in orange are all the points, sometimes called "inliers", that lie within this range; anything outside those lines—the dark-blue points—can be considered an outlier.</i></p>

<p>Note: There is no rigid mathematical definition of what constitutes an outlier; determining whether or not an observation is an outlier is ultimately a subjective exercise. The above rule is just one of many rules used. Another method often used is based on the interquartile range (IQR). For example, some people use the <span class='clFormula'>$1.5 \cdot \text{IQR}$</span>
rule. This defines an outlier to be any observation that falls <span class='clFormula'>$1.5 \cdot \text{IQR}$</span>
below the first quartile or any observation that falls <span class='clFormula'>$1.5 \cdot \text{IQR}$</span>
above the third quartile.</p>
<p>If we are to use the standard deviation rule, we can do this visually in the scatterplot by drawing an extra pair of lines that are two standard deviations above and below the best fit line. Any data points that are outside this extra pair of lines are flagged as potential outliers. Or, we can do this numerically by calculating each residual and comparing it to twice the standard deviation. Graphing calculators make this process fairly simple.</p>

<h3>Causes for Outliers</h3>

<p>Outliers can have many anomalous causes. A physical apparatus for taking measurements may have suffered a transient malfunction. There may have been an error in data transmission or transcription. Outliers arise due to changes in system behavior, fraudulent behavior, human error, instrument error or simply through natural deviations in populations. A sample may have been contaminated with elements from outside the population being examined. Alternatively, an outlier could be the result of a flaw in the assumed theory, calling for further investigation by the researcher.</p>
<p>Unless it can be ascertained that the deviation is not significant, it is ill-advised to ignore the presence of outliers. Outliers that cannot be readily explained demand special attention.</p>
<h2 id='section_42'>10.4. Expected Value and Standard Error</h2>

<h3 id='concept_201'>10.4.1. Expected Value</h3>

<blockquote>The expected value is a weighted average of all possible values in a data set.</blockquote>

<h4>Learning Objective</h4>

<p>Compute the expected value and explain its applications and relationship to the law of large numbers</p>

<h4>Key Points</h4>

<ul>
<li>The expected value refers, intuitively, to the value of a random variable one would "expect" to find if one could repeat the random variable process an infinite number of times and take the average of the values obtained.</li>
<li>The intuitive explanation of the expected value above is a consequence of the law of large numbers: the expected value, when it exists, is almost surely the limit of the sample mean as the sample size grows to infinity.</li>
<li>From a rigorous theoretical standpoint, the expected value of a continuous variable is the integral of the random variable with respect to its probability measure.</li>
</ul>

<h4>Key Terms</h4>

<dl>
<dt>weighted average</dt>
<dd>an arithmetic mean of values biased according to agreed weightings</dd>
<dt>integral</dt>
<dd>the limit of the sums computed in a process in which the domain of a function is divided into small subsets and a possibly nominal value of the function on each subset is multiplied by the measure of that subset, all these products then being summed</dd>
<dt>random variable</dt>
<dd>a quantity whose value is random and to which a probability distribution is assigned, such as the possible outcome of a roll of a die</dd>
</dl>

<p>In probability theory, the expected value refers, intuitively, to the value of a random variable one would "expect" to find if one could repeat the random variable process an infinite number of times and take the average of the values obtained. More formally, the expected value is a weighted average of all possible values. In other words, each possible value the random variable can assume is multiplied by its assigned weight, and the resulting products are then added together to find the expected value.</p>
<p>The weights used in computing this average are the probabilities in the case of a discrete random variable (that is, a random variable that can only take on a finite number of values, such as a roll of a pair of dice), or the values of a probability density function in the case of a continuous random variable (that is, a random variable that can assume a theoretically infinite number of values, such as the height of a person).</p>
<p>From a rigorous theoretical standpoint, the expected value of a continuous variable is the integral of the random variable with respect to its probability measure. Since probability can never be negative (although it can be zero), one can intuitively understand this as the area under the curve of the graph of the values of a random variable multiplied by the probability of that value. Thus, for a continuous random variable the expected value is the limit of the weighted sum, i.e. the integral.</p>

<h3>Simple Example</h3>

<p>Suppose we have a random variable <span class='clFormula'>$X$</span>
, which represents the number of girls in a family of three children. Without too much effort, you can compute the following probabilities:</p>

<div class='clFormula'>$P[X=0] = 0.125 \\&#10;P[X=1] = 0.375 \\&#10;P[X=2] = 0.375 \\&#10;P[X=3] = 0.125$</div>

<p>The expected value of <span class='clFormula'>$X, E[X]$</span>
, is computed as:</p>

<div class='clFormula'>$\displaystyle {\begin{align} E[X] &amp;= \sum_{x=0}^3xP[X=x] \\&#10;&amp;=0\cdot 0.125 + 1\cdot0.375 + 2\cdot 0.375 + 3\cdot 0.125 \\&#10;&amp;= 1.5\end{align}}$</div>

<p>This calculation can be easily generalized to more complicated situations. Suppose that a rich uncle plans to give you \$2,000 for each child in your family, with a bonus of \$500 for each girl. The formula for the bonus is:</p>

<div class='clFormula'>$Y = 1000 + 500X$</div>

<p>What is your expected bonus?</p>

<div class='clFormula'>$\displaystyle {\begin{align} E[1000 + 500X] &amp;= \sum_{x=0}^3 (1000 + 500x)P[X=x] \\&#10;&amp;=1000\cdot0.125 +1500\cdot0.375 + 2000\cdot0.375 + 2500 \cdot 0.125 \\&#10;&amp;= 1750 \end{align}}$</div>

<p>We could have calculated the same value by taking the expected number of children and plugging it into the equation:</p>

<div class='clFormula'>$E[1000+500X] = 1000 + 500E[X]$</div>


<h3>Expected Value and the Law of Large Numbers</h3>

<p>The intuitive explanation of the expected value above is a consequence of the law of large numbers: the expected value, when it exists, is almost surely the limit of the sample mean as the sample size grows to infinity. More informally, it can be interpreted as the long-run average of the results of many independent repetitions of an experiment (e.g. a dice roll). The value may not be expected in the ordinary sense—the "expected value" itself may be unlikely or even impossible (such as having 2.5 children), as is also the case with the sample mean.</p>

<h3>Uses and Applications</h3>

<p>To empirically estimate the expected value of a random variable, one repeatedly measures observations of the variable and computes the arithmetic mean of the results. If the expected value exists, this procedure estimates the true expected value in an unbiased manner and has the property of minimizing the sum of the squares of the residuals (the sum of the squared differences between the observations and the estimate). The law of large numbers demonstrates (under fairly mild conditions) that, as the size of the sample gets larger, the variance of this estimate gets smaller.</p>
<p>This property is often exploited in a wide variety of applications, including general problems of statistical estimation and machine learning, to estimate (probabilistic) quantities of interest via Monte Carlo methods.</p>
<p>The expected value plays important roles in a variety of contexts. In regression analysis, one desires a formula in terms of observed data that will give a "good" estimate of the parameter giving the effect of some explanatory variable upon a dependent variable. The formula will give different estimates using different samples of data, so the estimate it gives is itself a random variable. A formula is typically considered good in this context if it is an unbiased estimator—that is, if the expected value of the estimate (the average value it would give over an arbitrarily large number of separate samples) can be shown to equal the true value of the desired parameter.</p>
<p>In decision theory, and in particular in choice under uncertainty, an agent is described as making an optimal choice in the context of incomplete information. For risk neutral agents, the choice involves using the expected values of uncertain quantities, while for risk averse agents it involves maximizing the expected value of some objective function such as a von Neumann-Morgenstern utility function.</p>

<h3 id='concept_202'>10.4.2. Standard Error</h3>

<blockquote>The standard error is the standard deviation of the sampling distribution of a statistic.</blockquote>

<h4>Learning Objective</h4>

<p>Paraphrase standard error, standard error of the mean, standard error correction and relative standard error.</p>

<h4>Key Points</h4>

<ul>
<li>The standard error of the mean (SEM) is the standard deviation of the sample-mean's estimate of a population mean.</li>
<li>SEM is usually estimated by the sample estimate of the population standard deviation (sample standard deviation) divided by the square root of the sample size.</li>
<li>The standard error and the standard deviation of small samples tend to systematically underestimate the population standard error and deviations.</li>
<li>When the sampling fraction is large (approximately at 5% or more), the estimate of the error must be corrected by multiplying by a "finite population correction" to account for the added precision gained by sampling close to a larger percentage of the population.</li>
<li>If values of the measured quantity <span class='clFormula'>$A$</span>
are not statistically independent, an unbiased estimate of the true standard error of the mean may be obtained by multiplying the calculated standard error of the sample by the factor <span class='clFormula'>$f$</span>
.</li>
<li>The relative standard error (RSE) is simply the standard error divided by the mean and expressed as a percentage.</li>
</ul>

<h4>Key Terms</h4>

<dl>
<dt>correlation</dt>
<dd>One of the several measures of the linear statistical relationship between two random variables, indicating both the strength and direction of the relationship.</dd>
<dt>regression</dt>
<dd>An analytic method to measure the association of one or more independent variables with a dependent variable.</dd>
</dl>

<p>Quite simply, the standard error is the standard deviation of the sampling distribution of a statistic. The term may also be used to refer to an estimate of that standard deviation, derived from a particular sample used to compute the estimate. For example, the sample mean is the usual estimator of a population mean. However, different samples drawn from that same population would in general have different values of the sample mean. The standard error of the mean (i.e., of using the sample mean as a method of estimating the population mean) is the standard deviation of those sample means over all possible samples (of a given size) drawn from the population. Secondly, the standard error of the mean can refer to an estimate of that standard deviation, computed from the sample of data being analyzed at the time.</p>
<p>In regression analysis, the term "standard error" is also used in the phrase standard error of the regression to mean the ordinary least squares estimate of the standard deviation of the underlying errors.</p>

<h3>Standard Error of the Mean</h3>

<p>As mentioned, the standard error of the mean (SEM) is the standard deviation of the sample-mean's estimate of a population mean. It can also be viewed as the standard deviation of the error in the sample mean relative to the true mean, since the sample mean is an unbiased estimator. SEM is usually estimated by the sample estimate of the population standard deviation (sample standard deviation) divided by the square root of the sample size (assuming statistical independence of the values in the sample):</p>

<div class='clFormula'>$\displaystyle SE_\bar{x} = \frac{s}{\sqrt{n}}$</div>

<p>where:</p>
<ul>
<li>
<span class='clFormula'>$s$</span>
is the sample standard deviation (i.e., the sample-based estimate of the standard deviation of the population), and</li>
<li>
<span class='clFormula'>$n$</span>
is the size (number of observations) of the sample.</li>
</ul>
<p>This estimate may be compared with the formula for the true standard deviation of the sample mean:</p>

<div class='clFormula'>$\displaystyle SD_\bar{x} = \frac{\sigma}{\sqrt{n}}$</div>

<p>The standard error and the standard deviation of small samples tend to systematically underestimate the population standard error and deviations. This is due to the fact that the standard error of the mean is a biased estimator of the population standard error. Decreasing the uncertainty in a mean value estimate by a factor of two requires acquiring four times as many observations in the sample. Or decreasing standard error by a factor of ten requires a hundred times as many observations.</p>

<h3>Standard Error Versus Standard Deviation</h3>

<p>The standard error and standard deviation are often considered interchangeable. However, while the mean and standard deviation are descriptive statistics, the mean and standard error describe bounds on a random sampling process. Despite the small difference in equations for the standard deviation and the standard error, this small difference changes the meaning of what is being reported from a description of the variation in measurements to a probabilistic statement about how the number of samples will provide a better bound on estimates of the population mean. Put simply, standard error is an estimate of how close to the population mean your sample mean is likely to be, whereas standard deviation is the degree to which individuals within the sample differ from the sample mean.</p>

<h3>Correction for Finite Population</h3>

<p>The formula given above for the standard error assumes that the sample size is much smaller than the population size, so that the population can be considered to be effectively infinite in size. When the sampling fraction is large (approximately at 5% or more), the estimate of the error must be corrected by multiplying by a "finite population correction" to account for the added precision gained by sampling close to a larger percentage of the population. The formula for the FPC is as follows:</p>

<div class='clFormula'>$\displaystyle \text{FPC} = \sqrt{\frac{N-n}{N-1}}$</div>

<p>The effect of the FPC is that the error becomes zero when the sample size <span class='clFormula'>$n$</span>
is equal to the population size <span class='clFormula'>$N$</span>
.</p>

<h3>Correction for Correlation In the Sample</h3>

<p>If values of the measured quantity <span class='clFormula'>$A$</span>
are not statistically independent but have been obtained from known locations in parameter space <span class='clFormula'>$x$</span>
, an unbiased estimate of the true standard error of the mean may be obtained by multiplying the calculated standard error of the sample by the factor <span class='clFormula'>$f$</span>
:</p>

<div class='clFormula'>$\displaystyle f=\sqrt{\frac{1+\rho}{1-\rho}}$</div>

<p>where the sample bias coefficient <span class='clFormula'>$\rho$</span>
is the widely used Prais-Winsten estimate of the autocorrelation-coefficient (a quantity between <span class='clFormula'>$-1$</span>
and <span class='clFormula'>$1$</span>
) for all sample point pairs. This approximate formula is for moderate to large sample sizes and works for positive and negative <span class='clFormula'>$\rho$</span>
alike.</p>

<h3>Relative Standard Error</h3>

<p>The relative standard error (RSE) is simply the standard error divided by the mean and expressed as a percentage. For example, consider two surveys of household income that both result in a sample mean of \$50,000. If one survey has a standard error of \$10,000 and the other has a standard error of \$5,000, then the relative standard errors are 20% and 10% respectively. The survey with the lower relative standard error has a more precise measurement since there is less variance around the mean. In fact, data organizations often set reliability standards that their data must reach before publication. For example, the U.S. National Center for Health Statistics typically does not report an estimate if the relative standard error exceeds 30%.</p>
<h2 id='section_43'>10.5. Normal Approximation for Probability Histograms</h2>

<h3 id='concept_203'>10.5.1. Probability Histograms</h3>

<blockquote>A probability histogram is a graph that shows the probability of each outcome on the <span class='clFormula'>$y$</span>
-axis.</blockquote>

<h4>Learning Objective</h4>

<p>Explain the significance of a histogram as a graphical representation of data distribution</p>

<h4>Key Points</h4>

<ul>
<li>In a probability histogram, the height of each bar showsthe <em>true</em> probability of each outcome if there were to be a very large number of trials (not the <em>actual</em> relative frequencies determined by actually conducting an experiment).</li>
<li>By looking at a probability histogram, one can visually see if it follows a certain distribution, such as the normal distribution.</li>
<li>As in all probability distributions, the probabilities of all the outcomes must add up to one.</li>
</ul>

<h4>Key Terms</h4>

<dl>
<dt>independent</dt>
<dd>not dependent; not contingent or depending on something else; free</dd>
<dt>discrete random variable</dt>
<dd>obtained by counting values for which there are no in-between values, such as the integers 0, 1, 2, ….</dd>
</dl>

<h3>Histograms</h3>

<p>When examining data, it is often best to create a graphical representation of the distribution. Visual graphs, such as histograms, help one to easily see a few very important characteristics about the data, such as its overall pattern, striking deviations from that pattern, and its shape, center, and spread.</p>
<p>A histogram is particularly useful when there is a large number of observations. Histograms break the range of values in classes, and display only the count or percent of the observations that fall into each class. Regular histograms have a <span class='clFormula'>$y$</span>
-axis that is labeled with frequency. Relative frequency histograms instead have relative frequencies on the <span class='clFormula'>$y$</span>
-axis, with data taken from a real experiment. This chapter will focus specifically on probability histograms, which is an idealization of the relative frequency distribution.</p>

<h3>Probability Histograms</h3>

<p>Probability histograms are similar to relative frequency histograms in that the <span class='clFormula'>$y$</span>
-axis is labeled with probabilities, but there are some difference to be noted. In a probability histogram, the height of each bar shows the <em>true</em> probability of each outcome if there were to be a very large number of trials (not the <em>actual</em> relative frequencies determined by actually conducting an experiment). Because the heights are all probabilities, they must add up to one. Think of these probability histograms as idealized pictures of the results of an experiment. Simply looking at probability histograms make it easy to see what kind of distribution the data follow.</p>
<p>Let's look at the following example. Suppose we want to create a probability histogram for the discrete random variable <span class='clFormula'>$X$</span>
that represents the number of heads in four tosses of a coin. Let's say the coin is balanced, and each toss is independent of all the other tosses.</p>
<p>We know the random variable <span class='clFormula'>$X$</span>
can take on the values of 0, 1, 2, 3, or 4. For <span class='clFormula'>$X$</span>
to take on the value of 0, no heads would show up, meaning four tails would show up. Let's call this TTTT. For <span class='clFormula'>$X$</span>
to take on the value of 1, we could have four different scenarios: HTTT, THTT, TTHT, or TTTH. For <span class='clFormula'>$X$</span>
to take on a value of 2, we have six scenarios: HHTT, HTHT, HTTH, THHT, THTH, or TTHH. For <span class='clFormula'>$X$</span>
to take on 3, we have: HHHT, HHTH, HTHH, or THHH. And finally, for <span class='clFormula'>$X$</span>
to take on 4, we only have one scenario: HHHH.</p>
<p>There are sixteen different possibilities when tossing a coin four times. The probability of each outcome is equal to <span class='clFormula'>$\frac{1}{16}=0.0625$</span>
. The probability of each of the random variables <span class='clFormula'>$X$</span>
is as follows: </p>

<div class='clFormula'>$\displaystyle {P(X=0) = \frac{1}{16} = 0.0625 \\&#10;P(X=1) = \frac{4}{16} = 0.25 \\&#10;P(X=2) = \frac{6}{16} = 0.375 \\&#10;P(X=3) = \frac{4}{16} = 0.25 \\&#10;P(X=4) = \frac{1}{16} = 0.0625 \\}$</div>

<p>Notice that just like in any other probability distribution, the probabilities all add up to one.</p>
<p>To then create a probability histogram for this distribution, we would first draw two axes. The <span class='clFormula'>$y$</span>
-axis would be labeled with probabilities in decimal form. The <span class='clFormula'>$X$</span>
-axis would be labeled with the possible values of the random variable <span class='clFormula'>$X$</span>
: in this case, 0, 1, 2, 3, and 4. Then, rectangles of equal widths should be drawn according to their corresponding probabilities.</p>
<p>Notice that this particular probability histogram is symmetric, and resembles the normal distribution. If we had instead tossed a coin four times in many trials and created a relative frequency histogram, we would have gotten a graph that looks similar to this one, but it would be unlikely that it would be perfectly symmetric.</p>

<h3 id='concept_204'>10.5.2. Probability Histograms and the Normal Curve</h3>

<blockquote>Many different types of distributions can be approximated by the normal curve.</blockquote>

<h4>Learning Objective</h4>

<p>Assess normality using graphical tools to interpret data</p>

<h4>Key Points</h4>

<ul>
<li>The occurrence of the normal distribution in practical problems can be loosely classified into three categories: exactly normal distributions, approximately normal distributions, and distributions modeled as normal.</li>
<li>Just by looking at a probability histogram, you can tell if it is normal by looking at its shape. If the graph is approximately bell-shaped and symmetric about the mean, you can usually assume normality.</li>
<li>A normal probability plot is another method used to assess normality. The data are plotted against a theoretical normal distribution in such a way that, if the data is normal, the points should form an approximate straight line.</li>
</ul>

<h4>Key Terms</h4>

<dl>
<dt>central limit theorem</dt>
<dd>The theorem that states: If the sum of independent identically distributed random variables has a finite variance, then it will be (approximately) normally distributed.</dd>
<dt>normal probability plot</dt>
<dd>a graphical technique used to assess whether or not a data set is approximately normally distributed</dd>
</dl>

<p>When constructing probability histograms, one often notices that the distribution may closely align with the normal distribution. The occurrence of the normal distribution in practical problems can be loosely classified into three categories: exactly normal distributions, approximately normal distributions, and distributions modeled as normal.</p>

<h3>Exactly Normal Distributions</h3>

<p>Certain quantities in physics are distributed normally, such as:</p>
<ul>
<li>Velocities of the molecules in the ideal gas. More generally, velocities of the particles in any system in thermodynamic equilibrium will have normal distribution, due to the maximum entropy principle.</li>
<li>Probability density function of a ground state in a quantum harmonic oscillator.</li>
<li>The position of a particle that experiences diffusion.</li>
</ul>

<h3>Approximately Normal Distributions</h3>

<p>Approximately normal distributions occur in many situations, as explained by the central limit theorem. When the outcome is produced by a large number of small effects acting additively and independently, its distribution will be close to normal. The normal approximation will not be valid if the effects act multiplicatively (instead of additively), or if there is a single external influence that has a considerably larger magnitude than the rest of the effects.</p>
<p>The normal approximation can be used in counting problems, where the central limit theorem includes a discrete-to-continuum approximation and where infinitely divisible and decomposable distributions are involved. This includes Binomial random variables, which are associated with binary response variables, and Poisson random variables, which are associated with rare events.</p>

<h3>Assumed Normality</h3>

<p>There are many examples of problems in real life that are assumed to be normal. If you were to construct a probability histogram of these events with many trials, the histogram would appear to be bell-shaped. Examples include:</p>
<ul>
<li>Certain physiological measurements, such as blood pressure of adult humans.</li>
<li>Measurement errors in physical experiments. This use of a normal distribution does not imply that one is assuming the measurement errors are normally distributed, rather using the normal distribution produces the most conservative predictions possible given only knowledge about the mean and variance of the errors.</li>
<li>Results of standardized testing.</li>
</ul>

<h3>How to Assess Normality</h3>

<p>How can we tell if data in a probability histogram are normal, or at least approximately normal? The most obvious way is to look at the histogram itself. If the graph is approximately bell-shaped and symmetric about the mean, you can usually assume normality.</p>
<p>There is another method, however, than can help: a normal probability plot. A normal probability plot is a graphical technique for normality testing--assessing whether or not a data set is approximately normally distributed. The data are plotted against a theoretical normal distribution in such a way that the points should form an approximate straight line (, ). Departures from this straight line indicate departures from normality (, ). It is important to remember not to overreact to minor wiggles in the plot. These plots are not often produced by hand, but rather by technological tools such as the graphing calculator.</p>



<a href='../images/18362.png'><img class='clImageThumb' src='../images/18362.png' alt='Non-Normality - Histogram'></a>

<div><b><i>Non-Normality - Histogram</i></b></div>
<p><i>This is a sample of size 50 from a right-skewed distribution, plotted as a histogram. Notice that the histogram is not bell-shaped, indicating that the distribution is not normal.</i></p>



<a href='../images/18361.png'><img class='clImageThumb' src='../images/18361.png' alt='Non-Normality - Probability Plot'></a>

<div><b><i>Non-Normality - Probability Plot</i></b></div>
<p><i>This is a sample of size 50 from a right-skewed distribution, plotted as a normal probability plot. Notice that the points deviate on the, indicating the distribution is not normal.</i></p>



<a href='../images/18360.png'><img class='clImageThumb' src='../images/18360.png' alt='Approximately Normal - Histogram'></a>

<div><b><i>Approximately Normal - Histogram</i></b></div>
<p><i>This is a sample of size 50 from a normal distribution, plotted out as a histogram. The histogram looks somewhat bell-shaped, indicating normality.</i></p>



<a href='../images/18359.png'><img class='clImageThumb' src='../images/18359.png' alt='Approximately Normal - Probability Plot'></a>

<div><b><i>Approximately Normal - Probability Plot</i></b></div>
<p><i>This is a sample of size 50 from a normal distribution, plotted as a normal probability plot. The plot looks fairly straight, indicating normality.</i></p>

<h3 id='concept_205'>10.5.3. Conclusion</h3>

<blockquote>Many distributions in real life can be approximated using normal distribution.</blockquote>

<h4>Learning Objective</h4>

<p>Explain how a probability histogram is used to normality of data</p>

<h4>Key Points</h4>

<ul>
<li>In a probability histogram, the height of each bar shows the true probability of each outcome if there were a very large number of trials (not the actual relative frequencies determined by actually conducting an experiment).</li>
<li>The most obvious way to tell if a distribution is approximately normal is to look at the histogram itself. If the graph is approximately bell-shaped and symmetric about the mean, you can usually assume normality.</li>
<li>The normal probability plot is a graphical technique for normality testing. The data are plotted against a theoretical normal distribution in such a way that the points form an approximate straight line.</li>
<li>Many things in real life are approximately normally distributed, including people's heights and blood pressure.</li>
</ul>

<h4>Key Term</h4>

<dl>
<dt>normal probability plot</dt>
<dd>a graphical technique used to assess whether or not a data set is approximately normally distributed</dd>
</dl>

<h3>What is a Probability Histogram?</h3>

<p>It is often useful to display the data collected in an experiment in the form of a histogram. Having a graphical representation is helpful because it allows the researcher to visualize what shape the distribution takes.</p>
<p>Probability histograms are similar to relative frequency histograms in that the Y-axis is labeled with probabilities, but there are some differences to be noted. In a probability histogram, the height of each bar shows the <em>true</em> probability of each outcome if there were to be a very large number of trials (not the <em>actual</em> relative frequencies determined by actually conducting an experiment). Because the heights are all probabilities, they must add up to one. Think of these probability histograms as idealized pictures of the results of an experiment. Simply looking at probability histograms makes it easy to see what kind of distribution the data follow .</p>



<a href='../images/18136.png'><img class='clImageThumb' src='../images/18136.png' alt='Probability Histogram'></a>

<div><b><i>Probability Histogram</i></b></div>
<p><i>This probability histogram shows the probabilities that 0, 1, 2, 3, or 4 heads will show up on four tosses of a fair coin.</i></p>

<h3>How Can We Tell If the Data is Approximately Normal?</h3>

<p>The above example of a probability histogram is an example of one that is normal. How can we tell? The most obvious way is to look at the histogram itself. If the graph is approximately bell-shaped and symmetric about the mean, you can usually assume normality.</p>
<p>There is another method, however, than can help: a normal probability plot. A normal probability plot is a graphical technique for normality testing--assessing whether or not a data set is approximately normally distributed. The data are plotted against a theoretical normal distribution in such a way that the points form an approximate straight line . Departures from this straight line indicate departures from normality. It is important to remember not to overreact to minor wiggles in the plot. These plots are not often produced by hand, but rather by technological tools such as a graphing calculator.</p>



<a href='../images/18137.png'><img class='clImageThumb' src='../images/18137.png' alt='Normal Probability Plot'></a>

<div><b><i>Normal Probability Plot</i></b></div>
<p><i>The data points do not deviate far from the straight line, so we can assume the distribution is approximately normal.</i></p>

<h3>Approximately Normal Distributions in Real Life</h3>

<p>We study the normal distribution extensively because many things in real life closely approximate the normal distribution, including:</p>
<ul>
<li>The heights of people</li>
<li>The sizes of things produced by machines</li>
<li>Errors in measurements</li>
<li>Blood pressure</li>
<li>Scores on a standardized test</li>
</ul>

</div>
</div>
</body>
</html>
